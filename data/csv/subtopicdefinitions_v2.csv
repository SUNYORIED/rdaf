SUBTOPIC,DEFINITION,,,,,
E.1.1 Identification of goals and roles,"An exercise to define the objectives of, and responsible individuals for, various aspects of research data management (RDM).",,,,,
E.1.2 Vision and/or policy,"Vision: An aspirational state an organization wishes to achieve with respect to RDM.  Policy: A set of recommended and sometimes mandatory high-level principles that establish a guiding framework for RDM. [5,6]",,,,,
E.1.3 Data management organization ,"An RDM infrastructure (RDMI) of human and capital resources that supports data-related activities, e.g., policies, planning, and sharing, as well as practices and projects, e.g., data acquisition, control, and protection.",,,,,
"E.1.4 Organizational values, including DEI","A set of core beliefs that function as guides to what is seen as good and important in an organization and the guiding principles that provide an organization with purpose and direction. (Values ideally include diversity, equity, and inclusivity.) [10, 11] ",,,,,
E.1.5 Data management value proposition,A clear statement that indicates exactly what benefits an organization will derive from an RDM program. [12] ,,,,,
E.1.6 Data needs assessment,An evaluation of the types of data that are critical to accomplish the stated goals of an organization. ,,,,,
E.1.7 Purpose and value of data," A clear statement of the need for, use of, and benefit derived from, research data.  ",,,,,
E.1.8 Organization intent regarding FAIR data, The extent to which an organization supports the internal adoption and use of the FAIR data principles. ,,,,,
E.1.9 End-use support,"Components of the RDMI within an organization that enable data to be prepared and processed for its ultimate application, including reuse. ",,,,,
E.1.10 Stewardship,The application of rigorous analyses and oversight to ensure that data assets meet the needs of users. [13] ,,,,,
E.2.1 Privacy ,"The practice of protecting and properly handling sensitive data, including personal, proprietary, and confidential data",,,,,
E.2.2 Ethics ," Moral principles pertaining to data practices, e.g., analysis and dissemination, that have the potential to adversely impact people and society (e.g., to minimize bias and maintain the privacy of personal data). See also the Global Data Ethics Project. [15–17]",,,,,
E.2.3 Safety and security assurance ,"The practice of protecting digital information from unauthorized access, denial of access, theft, or corruption throughout its entire lifecycle. [18] ",,,,,
E.2.4 Inventory,"A function that provides organization capabilities for archiving management such that data products can be grouped, searched and identified for retrieval, statistics and reorganization. Also, a list of available items stored and/or controlled in a storage warehouse system. [15, pg 19] ",,,,,
E.2.5 Risk assessment,"A systematic process for the identification and evaluation of potential threats to and vulnerabilities of an organization’s data assets, e.g., unauthorized access to sensitive data.  [20] ",,,,,
E.2.6 Risk mitigation and management,"A process for the development and implementation  of appropriate strategies to control, reduce, or eliminate potential threats to and vulnerabilities of an organization’s data assets as identified by a risk assessment. [21] ",,,,,
E.2.7 Sharing/licensing,"Data sharing agreement: A formal contract that details what data are being shared and the appropriate use for the data. Licensing agreement: A formal contract that states the purpose and duration of access being provided to the recipient licensee along with restriction and security protocols the recipient licensee of the data must follow. [22, 23] ",,,,,
E.2.8 Social license for use and reuse,"An unwritten agreement whereby a group of public stakeholders accept that certain datasets may be applied for purposes other than those for which the data were originally intended, e.g., healthcare data. [24] ",,,,,
E.2.9 Jurisdiction for sharing and reuse,"Legal requirements as set by an authoritative entity (e.g., local and national regions) concerning the dissemination of data by an organization and subsequent use of the data by other organizations.  [25] ",,,,,
E.3.1 Roles and responsibilities,The job functions and obligations that enable the establishment of a desired data culture and reward structure. ,,,,,
E.3.2 Recognition of data management,Processes and practices that provide acknowledgement and rewards for good RDM at all levels in an organization. ,,,,,
E.3.3 Value of data workers,Recognition of the benefits that staff performing data-centric jobs or functions provide to an organization. ,,,,,
E.3.4 Promotion and tenure,"Career advancements that are linked to good research processes, practices, and outcomes. ",,,,,
E.3.5 Integrity of research and data,"For research: The condition resulting from adherence to professional values and practices when conducting, reporting, applying, and disseminating results of the work.[27] For data: The accuracy, completeness, and quality of data as they are maintained over time and across formats.[28]  ",,,,,
E.3.6 FAIR data principles," Guidelines that allow digital objects (e.g., data, algorithms, and workflows) to be Findable, Accessible, Interoperable, and Reusable. [29] ",,,,,
E.3.7 Maintainance of FAIR data, Ongoing infrastructural support to sustain FAIR data principles and practices. ,,,,,
E.3.8 Incentives and impact for sharing and reuse,Staff recognition and rewards for widespread dissemination and application of research data and the beneficial effects of such dissemination. ,,,,,
E.3.9 Disincentives for sharing and reuse,"Barriers that limit dissemination of data, e.g., misinterpretation and misuse of data by others, lack of recognition, and the effort required for sharing. ",,,,,
E.3.10 CARE and ethics,"The CARE (Collective benefit, Authority to control, Responsibility, and Ethics) Principles for Indigenous Data Governance are people and purpose-oriented, reflecting the crucial role of data in advancing Indigenous innovation and self-determination. (These principles complement the existing FAIR principles for indigenous data governance.)   Ethics: Moral principles pertaining to data practices, e.g., analysis and dissemination, that have the potential to adversely impact people and society (e.g., to minimize bias and maintain the privacy of personal data). [15, 30] ",,,,,
E.4.1 Workforce skills inventory, A catalog of an organization's capabilities in essential data processes. ,,,,,
E.4.2 Workforce preparedness in new and advanced technologies," Assessment of needs for and provision of training in the skills and expertise of an organization's staff pertinent to novel and leading-edge areas of research, e.g., AI. ",,,,,
E.4.3 Data management training  ,"In-classroom, on-line, and/or hands-on instruction for staff to attain the skills and expertise required to manage data across all lifecycles. ",,,,,
E.4.4 HR’s supporting role in workforce development and training,Involvement of an organization's HR department in establishing and implementing instructional courses for staff to expand their skill sets and expertise in research data programs and RDM.  ,,,,,
E.4.5 Promotional paths and career development," Documented approaches for recruitment, advancement, and retention of staff in data-centric jobs in an organization and expansion of data-related skills and expertise for all technical jobs.  ",,,,,
E.5.1 Sources of funding,"Entities that provide financial support for research data programs and RDM infrastructure (e.g., capital and human resources). ",,,,,
E.5.2 Long-term funding,Sustained financial support for research data activities and RDM infrastructure.,,,,,
E.5.3 Staffing,Provision of sufficient resources to support RDM staff and researchers engaged in RDM activities. ,,,,,
E.6.1 Stakeholder communities,"Individuals, groups, and organizations that have an interest or stake in RDM or research data activities in general and in particular domains. [31] ",,,,,
E.6.2 Modes of communication,Ways by which information about data and data management are shared and discussed. ,,,,,
E.6.3 Partners/partnerships,"Partner: Two or more organizations or individuals that share responsibility and control of ideas, processes, and outcomes of research data activities.  Partnership: An agreement between organizations and individuals to collaborate on such activities.. [32] ",,,,,
E.6.4 Engagement across knowledge domains and sectors,"Interactions among groups or individuals having expertise in different specific, specialized disciplines or fields, or expertise in different technology areas. [33] ",,,,,
E.6.5 Inclusivity in interactions, The practice of including all types of people or ideas and treating them all fairly and equally. [34] ,,,,,
E.6.6 Data services and the beneficiaries," Solutions for data tasks (e.g., data transfer, storage, and analytics) and the organizations or individuals deriving value from such solutions. [35] ",,,,,
P.1.1 Roles and responsibilities, The job functions and obligations for tracking data assets. ,,,,,
P.1.2 Implementation authority,"Person empowered to grant access to data assets, e.g., a Chief Data Officer.",,,,,
"P.1.3 Centralized inventory of services, groups, and resources"," An organization-wide catalog of elements supporting data-related activities at various levels of an organization, including capital (e.g., HPC), virtual (e.g., domain repositories), and human (e.g., Data Steward and AI interest group) components. ",,,,,
P.1.4 Provenance," The historical, documented record of a data asset that contains details on its origin—where, when, how, and by whom it was generated/acquired/processed—and on all alterations to the data asset. [15, pg 24,31] ",,,,,
P.2.1 Funding models for provisioning resources,"Approaches for providing financial support for data-related activities and infrastructure, including direct, (e.g., grants, contracts, and institutional), overhead, or mixed. [38] ",,,,,
P.2.2 Funding sources,"Entities that provide financial support for research data activities and infrastructure, e.g., capital and human resources. ",,,,,
P.2.3 Decision-making tools to assess costs,"Methods to determine the financial requirements of various data activities and infrastructure (e.g., cost-benefit analysis, market analysis, and decision trees). ",,,,,
P.2.4 Cost-benefit analysis,A systematic approach to estimating the strengths and weaknesses of alternative actions to determine options which provide the best approach to achieving benefits while preserving savings. [39] ,,,,,
P.2.5 Cost breakdown by lifecycle stage,"Identification of funds required for each data activity in a project (e.g., hardware, software, and staffing for data generation), or for an RDM infrastructure (e.g., centralized data services).",,,,,
P.2.6 Downstream lifecycle costs,"Funds required after establishment of an RDM infrastructure (e.g., technology refresh and maintenance) or for later-stage data activities (e.g., long-term preservation).  ",,,,,
P.2.7 Staffing and training,Costs incurred in assuring that new staff with appropriate skills and expertise are hired for specific data activities and that existing staff attain new and advanced skills through instructional courses. ,,,,,
P.3.1 Written data management plans (DMPs),"Documents that provide sufficient detail on the following topics: Administrative Data, Data Collection, Documentation and Metadata, Ethics and Legal Compliance, Storage and Backup, Selection and Preservation, Data Sharing, and Responsibilities and Resources. DMPs are referred to herein as a research data ""roadmaps."" [40] ",,,,,
P.3.2 Purpose/intent of research study and context of anticipated data use, Clear articulation of research objectives in terms of data products that are essential to address specific research and/or technical requirements. ,,,,,
"P.3.3 Specification of data objects, metadata, analysis tools, and workflows throughout the lifecycle","Detailed descriptions of all information, processes, software, and hardware required from conception to completion of a research data project. ",,,,,
P.3.4 Machine-readable DMPs,Research data roadmap documents in a form that can be used and understood by a computer. [41] ,,,,,
P.3.5 Linkage of DMPs to administrative records,"Interconnection of a research data roadmaps to operational data, e.g., agreements, transactions. ",,,,,
"P.3.6 Data organization in, e.g., a database and a repository, to facilitate future access","The practice of categorizing, classifying, and storing data with sufficient detail and specificity such that the data are readily discoverable and usable by others. [42] ",,,,,
P.3.7 Data management expertise and training,"In-class, on-line, and/or hands-on instruction for staff to attain the skills and knowledge required to manage data in a research study. ",,,,,
P.4.1 Quantitative and qualitative,"Quantitative data are numerical data, e.g., measurements and some controlled observations and questionnaires. Qualitative data are defined as non-numerical data, e.g., text, video, photographs, or audio recordings. [43]",,,,,
"P.4.2 Measurements, including images, audio recordings, and photos/videos","A quantity in various formats, including numerical, visual, and auditory. ",,,,,
P.4.3 Observation,A fact or occurrence often involving measurement with instruments. [44] ,,,,,
P.4.4 Survey,A list of questions aimed for extracting specific data from a particular group of people. [45] ,,,,,
P.4.5 Software,A computer-based application that converts inputs into outputs to support the user in one or more research tasks. [46] ,,,,,
P.4.6 Model,"A representation, pattern, or mathematical description that can help scientists replicate a system, process, or research result. [47] ",,,,,
P.4.7 Documentation (text),"Comprehensive information that accompanies a dataset, including all associated metadata, data dictionary, description of methods and instruments [and software used to generate/collect and] process the data, and other supporting data (e.g., duplicate sample results, replicate analyses). [48] ",,,,,
P.4.8 Specimen (physical sample),A tangible object that may observed or tested to determine its properties or characteristics. ,,,,,
P.4.9 Presentation,Material assembled to explain and describe research results or processes to an audience. ,,,,,
P.5.1 Organizational support for making data more FAIR," Institutional resources to improve the extent of ""FAIRness"" of data. (FAIRness is used herein to denote a continuum state ranging from no FAIR aspects to fully FAIR.) ",,,,,
P.5.2 Identification of methods/guidelines vis-à-vis FAIR principles,An exercise to locate techniques and recommended procedures related to FAIRness. ,,,,,
P.6.1 Criteria for selection of data/metadata,"Requirements and needs by which decisions are made regarding what information to generate, collect, and document in a research study. ",,,,,
P.6.2 Nature of data/metadata required,Specification of the requisite types and characteristics of selected information. ,,,,,
P.6.3 Intended extent of FAIRness, The degree to which data and metadata are meant to comply with the FAIR data principles. ,,,,,
P.6.4 Methods to capture and store data/metadata,"Techniques or means by which data/metadata are collected, recorded, and preserved.",,,,,
P.6.5 Metadata schema," The overall structure of data about the data. Two examples of general-purpose metadata schema are Dublin Core and MODS (Metadata Object Description Schema). [50, 51] ",,,,,
P.7.1 Design,"A set of principles that are formulated from specific strategies, rules, models, and guidelines for the management and flow of a dataset throughout its lifecycle. ",,,,,
P.7.2 Processing operations,"Methodology for translating raw data into useable information. Specific methods include, e.g., data preparation, validation, sorting, aggregation, analysis, and reporting.",,,,,
P.7.3 Workflow,"The process of managing data in a structured manner. It involves collecting, organizing, and processing data so that it can be used for various purposes. [52] ",,,,,
P.7.4 Model," A detailed description or scaled representation of the relationships and data flow between different components of an RDM system, typically in the form of a diagram or flowchart. [53] ",,,,,
P.7.5 LIMS," A laboratory information management system (LIMS) is a software system developed to support laboratory operations (e.g., track specimens and workflows and aggregate data). [54] ",,,,,
"P.7.6 Hosting and storage, cloud storage","Methods whereby, and locations wherein, data are saved and from which data can be retrieved.  ",,,,,
P.7.7 Configuration management,"The actions of tracking and controlling changes in the hardware and software components, e.g., updates and version control. [55] ",,,,,
P.7.8 Interoperability among different architectures,"The capability to communicate, execute programs, or transfer data among different RDM systems in a useful and meaningful manner that requires the user to have little or no knowledge of the unique characteristics of those systems. [58]",,,,,
P.7.9 Security," Features of the architecture that protect data from unauthorized access, denial of access, corruption, or theft throughout their entire lifecycles. [18] ",,,,,
P.7.10 Existing standards," Standards relevant to data architecture, including schema (e.g., based on SQL, JSON), format (e.g., JSON, XML), and APIs (e.g., Google Search for the web). ",,,,,
P.8.1 Organizational research needs," Essential resources required to accomplish the objectives of research projects and RDM (e.g., centralized infrastructure, appropriate training, and support staff). ",,,,,
P.8.2 Tools to support data-related processes," Items, e.g., instruments, methods, utility software, and APIs, that enable research. ",,,,,
P.8.3 Models that connect infrastructure to data processes and workflow,A detailed description or scaled representation of the relationships between data tasks and movement and the hardware and software components in an RDMI. [53] ,,,,,
P.8.4 Interoperability," The capability to seamlessly communicate, execute programs, or transfer data among various functional components .that requires the user to have little or no knowledge of the unique characteristics of those components. [58] ",,,,,
P.8.5 Persistent instrument identifiers," Globally unique, persistent, and resolvable identifiers of operational scientific instruments enable research data to be persistently associated with such crucial metadata, helping to set data into context. The Research Data Alliance Working Group Persistent Identification of Instruments (PIDINST) developed a metadata schema and prototyped schema implementation and demonstrated the viability of the proposed solution in practice. [59]",,,,,
P.8.6 Sustainability of data vis-à-vis obsolete infrastructure, Concerns regarding the ability to reproduce and reuse data if the hardware and software components become outdated or non-functional. ,,,,,
P.8.7 Security and privacy considerations," The degree of protection of data from unauthorized access, denial of access, corruption, or theft provided by the hardware and software. [18] ",,,,,
P.8.8 Staff expertise and support staff,"Personnel with the appropriate skills and knowledge to maintain and update the hardware and software infrastructure as needed, and personnel to interface with researchers using the infrastructure. ",,,,,
"P.9.1 Criteria, i.e., general vs. domain-specific standards","Requirements and needs by which decisions are made regarding the type of research standard, i.e., broadly applicable or limited to a particular field of research. ",,,,,
P.9.2 Sources of standards/guidelines for data/metadata," Origins of accepted practices consisting of discrete, reusable components, e.g., data types, identifiers, schemas, and formats. Examples include the Dublin Core Metadata Initiative and Schema.org. [60] ",,,,,
P.9.3 Quality standards," Guidelines that provide sufficient information to allow all users to readily evaluate the degree of “fitness for purpose” of the data. Key data quality components include completeness, accuracy, integrity, consistency, and timeliness. [15, pg 26, 57] ",,,,,
P.9.4 Community-based standards/conventions ,"Community-based data and metadata standards are typically long-term endeavors with many different players and types of efforts. Such standards facilitate reuse of data integrative analysis and comparison to other datasets and linkage of data with other research products, such as scholarly material, algorithms and software. [63] ",,,,,
P.10.1 Goals/definition of success,Statement of project objectives; list of accomplishments demonstrating that these objectives were met.  ,,,,,
"P.10.2 Metrics for tracking use and impact measures, including reuse"," Quantitative and qualitative indicators of positive influence or outcomes, (e.g., number of citations of a dataset and anecdotal evidence of reuse of a dataset). [64] ",,,,,
P.11.1 Methods to share and reuse data/metadata,"Approaches to disseminate data/metadata and to facilitate reusability of data/metadata, (e.g., use of open repositories and maximizing the FAIRness of data). ",,,,,
P.11.2 Allocation of credit to project team members, Properly documenting and recognizing each team member's contributions to a project. [65] ,,,,,
P.11.3 Promotion of data to communities of interest," Modes to communicate the existence and location of datasets to targeted groups, e.g., special-topic data publications and presentations at topical workshops. ",,,,,
P.11.4 Cross-institution cooperation," The process of working with other institutions or organizations on a shared activity (e.g., informal collaborations, formal partnerships, and agreements). ",,,,,
P.11.5 Requests for additional data from community, Soliciting data contributions from partners and stakeholders on areas of mutual interest.  ,,,,,
P.12.1 Identification of responsible parties for access, A determination of those individuals authorized to both prohibit and permit access to sensitive data. ,,,,,
P.12.2 Ease of maintenance and implementation of records, The extent to which sensitive data can be kept up to date and made accessible to authorized individuals and groups. ,,,,,
P.12.3 Regulatory compliance," Efforts by organizations to ensure that they are aware of and take steps to conform to relevant laws, policies, and regulations concerning sensitive data (e.g., medical records). [66] ",,,,,
P.12.4 Sensitive data/PII, Data that needs to be controlled due to certain risks. Personally Identifiable Information (PII) is any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means. [67] ,,,,,
"P.12.5 Limited disclosure, e.g., IP"," Restricting release of data to specific legal circumstances and often requiring notification to the data provider. Intellectual Property (IP) refers to certain exclusive rights granted by law to the owner of, e.g., a novel data product. [68] ",,,,,
P.12.6 Licensing for reuse,Legal agreement that allows one party to use another party's data subject to certain conditions. ,,,,,
"GA.1.1 Measurements, including images, audio recordings, photos/videos","A quantity in various formats, including numerical, visual, and auditory.",,,,,
GA.1.2 Text file,"A type of digital, non-executable file that contains letters, numbers, symbols and/or a combination of these without any special formatting (e.g., ASCII and EBCDIC).",,,,,
"GA.1.3 Computation, simulation","Computation: an act, process, or method of computing. Simulation: any research or development project wherein a model of some authentic phenomenon is created to mimic outcomes that happen in the natural world.",,,,,
GA.1.4 Source code,A set of instructions and statements written by a programmer using a computer programming language. This code is later translated into machine language by a compiler.,,,,,
GA.1.5 Observation,A fact or occurrence often involving measurement with instruments.,,,,,
GA.1.6 Survey,A list of questions aimed at extracting specific data from a particular group of people.,,,,,
GA.1.7 Transaction,"Data that describe an exchange or transfer of goods, services, or funds.",,,,,
GA.1.8 Social media,"Interactive technologies that facilitate the creation and sharing of information (i.e., data) through virtual communities and networks.",,,,,
GA.2.1 In-house generation by researchers,Data created by researchers within an organization and at a physical location internal to the organization.,,,,,
GA.2.2 Remote generation by researchers,Data created by researchers within an organization through control of a instrument or device at a location other than the organization.,,,,,
GA.2.3 In-field generation by researchers,"Data created by researchers within an organization at a physical location external to the organization, which may be a natural environment.",,,,,
GA.2.4 User facility generation by/for researcher,Data created by researchers or facility staff at a federally sponsored research facility available for external use to advance scientific or technical knowledge.,,,,,
GA.2.5 Historical,"Data generated or collected in the past, which may have particular uncertainties due to, e.g., age and loss of metadata.",,,,,
GA.2.6 Human-annotated,"The process of adding metadata or other information in different formats to data by a person such as labels or tags to describe the content or context of images, and labels or tags to classify or extract relevant information from text. Such annotation allows AI and ML models to categorize data and approve the execution of relevant tasks.",,,,,
GA.3.1 Source of objects/subjects,Origin of items used in an experiment.,,,,,
GA.3.2 Characteristics of objects/subjects,"Distinct features of items used in an experiment, e.g., appearance and properties.",,,,,
GA.3.3 Conditions of research study,"Description of the external physical environment in which data were collected (e.g., temperature, atmosphere). Such conditions are types of metadata.",,,,,
GA.3.4 Specification of instruments and tools,"Identification and documentation of measurement equipment and other items, e.g., software, methods, and materials, used in an experimental research study. Includes descriptions of the technical details and requirements of each item.",,,,,
GA.3.5 Parameters for instruments and tools,"Variables or settings on an instrument or tool that are maintained and controlled during an experiment (e.g., laser intensity, gas flow rate, and rate of data collection).",,,,,
"GA.3.6 Methods, protocols, and calibration",Techniques and procedures used in the generation of data.,,,,,
GA.3.7 Data/metadata capture methods,"Techniques and procedures for collecting and recording information, for both short-term and long-term storage.",,,,,
GA.3.8 Provenance and capture methods,"Techniques and procedures for collecting and recording the historical, attributed, and documented record of a data asset that contains details on its origin-where, when, how, and by whom it was generated/acquired/processed-and on all alterations to the data asset.",,,,,
GA.3.9 Reproducibility,"The ability to replicate data using identical tools (e.g., documented metadata, code, methods, and instruments) employed previously by the original researchers or by other researchers, without the need for any additional information or communication with the original researchers.",,,,,
GA.4.1 Input data/metadata,"Information of any type that is entered manually or via an automated process into an instrument, computer, or other device.",,,,,
GA.4.2 Output data/metadata,"Electronic data produced by an instrument, processor, computer, or other device.",,,,,
GA.4.3 Hardware ,"The physical elements that make up a computer or electronic system and everything else involved that is physically tangible, including monitors, hard drives, memory, and the CPU.",,,,,
GA.4.4 Parameters and conditions for computation,"Hardware or software system requirements or configurations that are necessary for a hardware or software application to run smoothly and efficiently, e.g., operating system dependencies, compilers, and memory requirements.",,,,,
GA.4.5 Versioning ,"The process of numbering different releases of entities, e.g., software, hardware, and documents, for the purposes of tracking and recording changes. This provides the ability to revert to a previous revision, which is critical for data traceability and data re-creation, tracking edits, and correcting errors.",,,,,
GA.4.6 Data/metadata capture methods,Techniques and procedures by which information is collected and recorded.,,,,,
GA.4.7 Provenance and capture methods,"Techniques and procedures for collecting and recording the historical, attributed, and documented record of a data asset that contains details on its origin-where, when, how, and by whom it was generated/acquired/processed-and on all alterations to the data asset.",,,,,
GA.4.8 Verification/validation of output data,Verification: the process of determining that a computational model accurately represents the underlying mathematical model and its solution. Validation: the process of determining the degree to which a model is an accurate representation of the real world from the perspective of the intended uses of the model.,,,,,
GA.5.1 Nature of objects/subjects,Types and characteristics of entities which are being studied.,,,,,
GA.5.2 Methods and protocols,"Techniques, standard operating procedures, sets of rules, and guidelines.",,,,,
GA.5.3 Metadata,"Data about data, i.e., data that define and describe the characteristics of other data. Using a survey as an example, metadata include the questions in, and the location of, the survey.",,,,,
GA.5.4 Paradata,"Data about the process by which data were collected. Formalized data on methodologies, processes, and quality associated with the production and assembly of statistical data. Using a survey as an example, paradata include the mode of the survey and responders' response times. Note that paradata are typically associated with social science disciplines; in physical and medical science disciplines, paradata would be included in metadata.",,,,,
GA.5.5 Data/metadata/paradata capture methods,"Techniques and procedures for collecting and recording any type of data, either manually or via an automated process using an instrument, computer, or other device.",,,,,
GA.6.1 From collaborators,Originating from other individuals or other organizations partnering with researchers in an organization.,,,,,
GA.6.2 From repositories,"Originating from a destination designated for data storage. Operations of a repository include preservation, management, and provision of access for digital materials that may have different types and formats.",,,,,
GA.6.3 From the literature,Originating from a publication.,,,,,
GA.6.4 Aggregated datasets from multiple sources,Data compiled from disparate studies that are organized and summarized so that conclusions can be drawn and decisions made from such data-rich collections.,,,,,
GA.6.5 Provenance,"The historical, attributed, and documented record of a data asset that contains details on its origin-where, when, how, and by whom it was generated/acquired/processed-and on all alterations to the data asset.",,,,,
"GA.6.6 Restrictions, fees, and usage agreements ",Mechanisms that may limit the use of acquired data.,,,,,
GA.7.1 Infrastructure to assure the greatest data integrity,"A foundation composed of practices, processes, and procedures designed to produce data that are clean, traceable, and fit for purpose. NIST and KRISS are two institutions that produce critically evaluated data named Standard Reference Data.",,,,,
GA.7.2 Single researcher dataset,A group of data that originates from an individual researcher.,,,,,
GA.7.3 Aggregation of data evaluated by experts,"The process by which data from disparate sources are compiled, reviewed, critiqued, and summarized by subject-matter experts.",,,,,
GA.7.4 Reproducibility and uncertainty quantification,"Reproducibility: the ability to replicate data using identical tools (e.g., documented metadata, code, methods, and instruments) employed previously by the original researchers or by other researchers without the need for any additional information or communication with the original researchers. Uncertainty quantification: Assignment of a numerical value to a non-negative parameter characterizing the dispersion of the quantity values being attributed to a measurand. Critically evaluated data have great reproducibility and small uncertainty.",,,,,
GA.7.5 Intellectual property rights,"Legally enforceable claims for owners of original ideas, inventions, and creative expressions. For intellectual property (IP), any agreement must include an assessment of what IP rights subsist in the data, who owns them, what exceptions or limitations apply, and any contractual rights or policies related to IP that should be considered within the data governance framework, including acquired and generated data as well as “background” (i.e., pre-existing) or “foreground” (i.e., from original research) IP.",,,,,
GA.8.1 Data born FAIR,Data objects that comply with the FAIR principles when first generated or produced.,,,,,
GA.8.2 Data made FAIR,Data objects that are transformed or changed in some manner so that they comply with the FAIR principles.,,,,,
GA.8.3 FAIR digital objects,"Standardized, autonomous, and persistent entities which contain the information needed about different kinds of digital objects (e.g., data, metadata, documents, software, and semantic assertions), to enable both humans and machines to Find, Access, Interoperate, and Reuse (FAIR) these digital objects in highly efficient and cost-effective ways.",,,,,
GA.8.4 FAIR on a continuous scale,"Recognition that there is a degree of FAIRness that ranges from fully FAIR to not FAIR, that may be represented on a numerical scale.",,,,,
"GA.8.5 Guidelines/methodologies for each aspect: F, A, I, R","Means, e.g., standards, best practices, protocols, and software, by which the findability, accessibility, interoperability, and reusability of data may be improved.",,,,,
GA.8.6 Tools to capture FAIR provenance ,"Techniques and procedures for collecting and recording the collective information on the FAIRness of a data asset, from its origin to the present.",,,,,
GA.8.7 FAIR instruments and tools,"Equipment, devices, methods, standards, and other tools that enable the findability, accessibility, interoperability, and reusability of data (e.g., SmartAPI).",,,,,
"GA.8.8 Not FAIR data (e.g., legacy data)","Data that are not findable, accessible, interoperable, and reusable to any degree for various reasons, e.g., obtained using old or obsolete instruments or software.",,,,,
GA.9.1 General vs. domain-specific,Broadly applicable as opposed to limited to a particular field or area.,,,,,
GA.9.2 Standards development organizations vs. community consensus,"Formal, recognized, standards bodies (e.g., ISO and ASTM International), as opposed to informal, self-assembled groups of individuals or institutions with shared interests (e.g., professional societies).",,,,,
GA.9.3 Data format and file structure,"Data format: the organization of data according to preset specifications. File structure: The manner by which data and code are organized within a file with the goal of reusability. In the context of standards, the syntax, encoding, and file format or media type for storing or transmitting data (e.g., CVS and JSON).",,,,,
GA.9.4 Metadata format and file structure,"Metadata format: the organization of information (i.e., metadata) according to preset specifications. File structure: The manner by which metadata are organized within a file. In the context of standards, a metadata standard is a high-level document which establishes a common way of structuring and understanding data, and includes principles and implementation guidance for utilizing the standard. See the RDA Metadata Standards Catalog.",,,,,
GA.9.5 Vocabulary and ontology,"Vocabulary: a compendium of standardized terms with consistent semantic definitions. Ontology: a description of data structure (e.g., classes, properties, and relationships) in a domain of knowledge.",,,,,
GA.9.6 Interoperability,"The capability to seamlessly communicate, execute programs, or transfer data among various functional components that requires the user to have little or no knowledge of the unique characteristics of those components. Interoperability standards enable the operational processes underlying exchange and sharing of information between different systems to ensure that all digital research outputs are Findable, Accessible, Interoperable and Reusable, according to the FAIR principles.",,,,,
GA.10.1 Open source vs. proprietary,"Programs freely distributed with the source code that researchers can modify and subsequently redistribute modified versions thereof vs. programs that are copyrighted and bear limits against use, distribution and modification that are imposed by their publisher, vendor, or developer. Such programs remain the property of their owner/creator and are used by end-users/organizations under predefined conditions.",,,,,
GA.10.2 LIMS,"A laboratory information management system (LIMS) is a software system developed to support laboratory operations, e.g., track specimens and workflows, and collect, annotate, and aggregate data.",,,,,
GA.10.3 Instrument control,Software for configuring the operating parameters of an instrument.,,,,,
GA.10.4 Electronic laboratory notebooks ,"A software tool that digitally replicates paper laboratory notebooks traditionally used in the sciences to record information on observational, experimental, and computational studies.",,,,,
GA.10.5 Audio and video recordings,A digital record used to store and preserve the audible and/or visual components of an event.,,,,,
"PA.1.1 Tables, spreadsheets","Tables: numerical and textual information arranged in rows and columns. Spreadsheets: computer programs that can capture, display and manipulate data arranged in rows and columns.",,,,,
"PA.1.2 Charts, graphs","Visual representations of datasets, e.g., diagrams, pictures, and graphs. Graphical charts show mathematical relationships between varied groups of data.",,,,,
"PA.1.3 Maps, vectors, images","Representations of the relationships between variables, i.e., quantities, phenomena, or entities. Maps: diagrammatic depictions of the association of two or three variables. Vectors: linear depictions of two independent variables. Images: visual depictions of an object in two or three dimensions.",,,,,
PA.1.4 Instrument outputs,"Raw electronic data generated by a piece of equipment, device, or other tool before any human action on the data or processing of the data.",,,,,
PA.1.5 Dynamic data,Data which are changing frequently and at asynchronous moments. Data that may change after they are recorded and have to be continually updated.,,,,,
PA.1.6 Datasets from models and simulations,"Organized collections of data generated by models (i.e., representations, patterns, or mathematical descriptions that can help scientists replicate a system, process, or research result) and simulations (i.e., creation of a model of some authentic phenomenon to mimic outcomes that happen in the natural world).",,,,,
"PA.1.7 Structured data, e.g., hierarchical organization","Data whose elements have been organized (e.g., hierarchical) into a consistent format and data structure within a defined data model such that the elements can be easily addressed, organized, and accessed in various combinations to make better use of the information (e.g., a relational database).",,,,,
PA.2.1 Data cleaning,"The process of detecting and correcting corrupt or inaccurate records in a dataset. This process involves identifying, replacing, modifying, or deleting incomplete, incorrect, inaccurate, inconsistent, irrelevant, and improperly formatted data.",,,,,
"PA.2.2 De-identification, anonymization","A process by which personal data are irreversibly altered in such a way that the person can no longer be identified directly or indirectly, either by the data controller alone or in collaboration with any other party.",,,,,
PA.2.3 Amputation and imputation,"Amputation: a process whereby some valid data points are selectively deleted from a complete dataset. Imputation: a process used to determine and assign replacement values for missing, invalid, or inconsistent data.",,,,,
PA.2.4 Aggregation,"A process used to combine datasets, typically taken collectively or in the form of a summary. Integration of data by aggregation requires data interoperability, harmonization, and mapping.",,,,,
PA.2.5 Validation and verification,Validation: the process of determining the degree to which a model is an accurate representation of the real world from the perspective of the intended uses of the model. Verification: The process of determining that a computational model accurately represents the underlying mathematical model and its solution.,,,,,
PA.2.6 Curation,"The ongoing processing and maintenance of data throughout their lifecycle to ensure long-term accessibility, sharing, and preservation. Data curation is composed of research data management and digital preservation and involves processes such as the addition of metadata to make data more findable and understandable, ingestion of data into a repository, validation of file checksums and file fixity checks, and other tasks for organizing, cleaning, describing, enhancing, storing, and preserving data.",,,,,
PA.2.7 Normalization of metadata,The adjustment of metadata elements into standard formats.,,,,,
PA.3.1 Manual,"Collection, organization, and transformation of data by a human without using a machine or any other tool.",,,,,
PA.3.2 Exploratory,"Techniques that typically use visual tools to, e.g., determine the main characteristics of datasets, find relationships among datasets or variables that may have been unknown or overlooked, and discern trends or differences among datasets.",,,,,
PA.3.3 Descriptive,"Techniques for answering the question, ""What happened?"", e.g., identification of trends and relationships using current and historical (past) data.",,,,,
PA.3.4 Diagnostic,"Techniques for answering the question, ""Why did this happen?"", e.g., determination of the causes of trends and correlations among datasets or variables.",,,,,
PA.3.5 Evaluative,"Techniques for systematic determination of merit, worth, value, or significance of datasets, e.g., relevance to the project objectives.",,,,,
PA.3.6 Predictive,"Techniques for answering the question, ""What might happen in the future?"", e.g., making assumptions about the future using historical data, either manually or with machine-learning algorithms.",,,,,
PA.3.7 Prescriptive ,"Techniques for answering the question, ""What should we do next?"", e.g., informing an optimal course of action, decisions, and strategies, often via machine learning.",,,,,
PA.3.8 Correlational,"Techniques that provide a statistical measure indicating how strongly two variables are related and whether that relationship is positive (e.g., when one variable increases, the other also increases) or negative (e.g., when one variable increases, the other decreases).",,,,,
PA.3.9 Statistical ,"Techniques whereby data are interpreted to uncover patterns and trends. The five basic statistical techniques are mean, standard deviation, regression, hypothesis testing, and sample size determination.",,,,,
"PA.3.10 Automated, autonomous","Techniques that require no human guidance or direct intervention and are based solely on machines, e.g., self-driving vehicles.",,,,,
PA.4.1 Visualization,"Techniques for the representation of data (e.g., graphs, images, and diagrams). Transformation of numerical data into a visual or pictorial context in order to assist users in better understanding what the data mean.",,,,,
"PA.4.2 ML, AI","Machine learning (ML) is a methodology that uses statistics and mathematical models to detect patterns in historical data and learning algorithms to make predictions about new data. Artificial intelligence (AI) is a field of study in which computerized systems can learn, solve problems, and autonomously achieve goals under varying (and sometimes uncertain) conditions. ML is a subset of AI strategies.",,,,,
PA.4.3 Iterative model fitting,A technique whereby the parameters of a model are adjusted in repeated cycles to improve accuracy of the computation.,,,,,
"PA.4.4 Integrated development environments, e.g., Jupyter, RStudio","An application that facilitates application development, typically via a graphical user interface (GUI)-based workbench designed to build software applications in combination with all the required tools, e.g., Jupyter and Rstudio. Common features include, e.g., debugging, version control, and data structure browsing.",,,,,
PA.5.1 Types of metadata,"The three main categories or classifications of metadata are descriptive, structural, and administrative.",,,,,
PA.5.2 Responsible parties,"Individuals whose duties or job functions include the management of metadata, e.g., data owner or metadata steward.",,,,,
PA.5.3 Specification of metadata standards ,"Identification and description of those metadata standards categorized as four types: format/technical interchange, structure, content, and value. Standards include recommended practices, classifications, test methods, and guides.",,,,,
PA.5.4 Linked data structure,"A deliberate design for the organization of data (structure) wherein information (metadata) is brought together from different sources (linked) in order to create a new, richer dataset.",,,,,
"PA.5.5 Persistent identifiers (e.g., DOI, ORCID, ARK, ROR, PIDINST, Handles)","A unique and long-lasting reference that allows for continued access to an entity (e.g., document, dataset, instrument, webpage, contributor, and organization). A persistent identifier (PID) may be connected to a set of metadata describing an object rather than to the object itself. Examples of PIDs include DOI, ORCID, ARK, ROR, PIDINST, and Handles.",,,,,
PA.6.1 Original authoritative copy,"The single, distinct, absolute version of a dataset from the originating source that is unique, identifiable, and unalterable without detection. It should be sufficient to allow a third party to reproduce the results of the research.",,,,,
PA.6.2 Version identification,"For a specific time, definitive determination of a previous dataset made possible by comprehensive information (e.g., raw data, computer code, software, and documentation) on that dataset. Such an ability to revert to previous versions is critical for data traceability, tracking edits, and correcting mistakes.",,,,,
PA.6.3 Derivative product,"Any data, publication, illustration or visualization, or other work that rearranges, presents, or otherwise makes use of an existing dataset.",,,,,
PA.6.4 Aggregation,"A process used to combine datasets, typically resulting in a collection or summary.",,,,,
PA.6.5 Subset,A portion of a dataset that is referentially intact.,,,,,
PA.6.6 Timestamp,Temporal information regarding an event that is recorded by a computer and then stored as a log or metadata.,,,,,
PA.6.7 CRediT taxonomy,"Contributor Roles Taxonomy (CRediT) consists of a high-level taxonomy, including 14 roles, that can be used to represent the roles typically played by contributors to research outputs.",,,,,
PA.7.1 Commercial vs custom,"Commercial software is any software or program designed and developed for licensing or sale to end-users or for serving a commercial purpose (e.g., off-the-shelf programs and games). Custom software is made for an individual or organization and performs tasks specific to their needs.",,,,,
PA.7.2 Open-source vs proprietary,"Open source typically refers to software that is freely distributed with source code that can modified by users and modified versions may be redistributed. Proprietary typically refers to software that is copyrighted and bears limits against use, distribution, and modification that are imposed by its publisher, vendor or developer. The software remains the property of its owner/creator and is used by end-users/organizations under predefined conditions.",,,,,
PA.7.3 Aggregation tools,Software or programs that enable the combination of datasets.,,,,,
PA.7.4 Surveying tools,Software or programs that aid in the gathering of responses to questions aimed at extracting specific data from a particular group.,,,,,
PA.7.5 Statistical tools,"Software or programs used in statistics, i.e., the collection, organization, analysis, interpretation, and presentation of masses of data.",,,,,
PA.7.6 Calculation and analysis tools,"Software or programs that produce knowledge from organized data to draw conclusions, highlight useful information, and support decision-making.",,,,,
PA.7.7 APIs,"An Application Programming Interface (API) is a set of protocols, routines, functions and/or commands that programmers use to facilitate interactions between distinct software services.",,,,,
PA.7.8 Database management tools,"Software or programs that aggregate diverse data into a database or other consistent resource, handle different types of queries, provide security, and perform other functions.",,,,,
PA.7.9 Testing and validation tools,"Methods to determine if software or programs perform the functions for which they were designed. Software or programs that help ensure that the data sent to connected applications are complete, accurate, secure, and consistent.",,,,,
PA.7.10 Documentation,"Written information that describes the software product to the people who develop, deploy and use it, including technical manuals and online material, such as online versions of manuals and help capabilities. The term is sometimes used to refer to source information about the product discussed in design documentation, code comments, white papers and session notes.",,,,,
PA.7.11 Reproducibility and uncertainty quantification,"Reproducibility: the ability to replicate data using identical tools (e.g., documented metadata, code, methods, and instruments) employed previously by the original researchers or by other researchers without the need for any additional information or communication with the original researchers. Uncertainty quantification: assignment of a numerical value to a non-negative parameter characterizing the dispersion of the quantity values being attributed to a measurand.",,,,,
PA.7.12 Versioning and maintenance,"The process of numbering different releases of a particular software program for both internal use and release designation. This process allows programmers to know when changes have been made and track changes enforced in the software. At the same time, it enables potential customers to be acquainted with new releases and recognize the updated versions.",,,,,
PA.7.13 Systems resilience and adaptability,"Resilience: the ability of a software system to continue to operate under adverse conditions while maintaining essential operational capabilities, and to recover to an effective operational state in an acceptable time frame. Adaptability: the ability of a software system to tolerate changes in its environment without external intervention.",,,,,
PA.7.14 Source code repository,"A storage location for source code (the fundamental component of a computer program) that holds code, makes code available for use, and organizes code in a logical manner.",,,,,
PA.7.15 Security and software updates,"Patch, upgrade, or other modification to code that corrects security and/or functionality problems in software.",,,,,
"PA.7.16 Standards, protocols, and interfaces ","Standards: codes, programs, and associated documentation that describe how data should be stored or exchanged for the consistent collection and interoperability of that data across different systems, sources, and users. Protocols: sets of rules and guidelines. Interfaces: programs that allow a user to interact with computers in person or over a network, or the controls used in a program that allow the user to interact with the program.",,,,,
PA.8.1 LIMS,"A laboratory information management system (LIMS) is a software system developed to support laboratory operation (e.g., track specimens, collect and annotate data and workflows, and aggregate datasets).",,,,,
"PA.8.2 Laboratory notebooks, e.g., electronic and paper","A complete, detailed record of the hardware, software, procedures, materials, observations, and relevant thought processes for the research that would enable the work and resulting data to be reproducible. This typically includes an explanation of why the research was done, including any necessary background and references, how the research was performed, the actual data (raw and processed), and where the data are stored. Laboratory notebooks may be paper or electronic.",,,,,
PA.8.3 Tools for automated metadata capture,"Software, hardware, and methods used to collect and record data about data without the need for manual instruction.",,,,,
PA.8.4 Anomaly detection and correction tools,"Software, hardware, and methods used to identify items (e.g., operations, observations, events, and results), that do not conform to the expected pattern or result (i.e., anomaly detection) and to restore such items to the expected pattern or result (i.e., anomaly correction).",,,,,
PA.8.5 Collaboration tools,"Software and/or software systems that enable communication and sharing of documents, data, analyses, and/or visualizations amongst individuals who are not co-located.",,,,,
PA.8.6 Decisions regarding the need for additional data,Conclusions by researchers that more data are needed to accomplish project goals.,,,,,
PA.8.7 Process monitoring and evaluation,Periodic tracking of the operation and results of a workflow component by systematically gathering and analyzing data to assure that the component is functioning properly.,,,,,
PA.8.8 Containerization,"Operating system-level virtualization or application-level virtualization over multiple network resources so that software applications can run in isolated user spaces called containers in any cloud or non-cloud environment, regardless of type or vendor.",,,,,
PA.8.9 Reusable workflow component,"A discrete piece of software that can be compiled and packaged as an activity and reused in multiple processes, thereby reducing duplication and enabling sharing of the software with others.",,,,,
PA.8.10 Microservices,"An approach to software development in which a large application is built from modular software components (i.e., microservices), each of which does one defined job (e.g., messaging).",,,,,
PA.8.11 Distributed workflow across sites,"Computerized information system that is responsible for scheduling and synchronizing the various tasks within the workflow across physical or virtual locations, in accordance with specified task dependencies, and for sending each task to the respective processing entity.",,,,,
PA.8.12 Comprehensive report generation,"The production of a single document which includes all of the information needed to reproduce a dataset, including, e.g., methods, format standards, and software versions.",,,,,
PA.9.1 Compute requirements,"Specifications of the raw processing power of a computer to meet the needs for activities, applications, or workloads. Such power may be characterized as the rate at which operations are performed, e.g., million instructions per second (MIPS).",,,,,
PA.9.2 Storage requirements,"Specifications and needs for devices and components that store data on a long-term basis for later uses and access (e.g., hard disks and network-attached storage devices). In contrast to storage, memory is the short-term location for temporary data storage.",,,,,
PA.9.3 Network requirements,"Network capability is characterized by stability of the signal, throughput (transfer rate of data from a source system to a destination system), and bandwidth (the amount of data that can be transferred per second, in megabits/sec).",,,,,
PA.9.4 Accelerator requirements,"Specifications and needs for hardware devices...designed to improve the overall performance of the computer. Hardware acceleration is a process where applications offload certain [computing] tasks to [specialized] hardware components within the system, enabling greater performance and efficiency.",,,,,
"SUR.1.1 Repository, i.e., domain, generalist, institutional","A broad term that refers to a designated location where a collection of digital objects is stored in an organized manner such that the collection is findable, searchable, accessible, and reusable. Types of repositories include domain-specific (e.g., discipline or subject matter); generalist (a variety of data types, format, and content); and institutional (i.e., within an organization).",,,,,
SUR.1.2 Data papers,"A publication that contains datasets, without having to be at the stage of presenting further analysis and conclusions as in a traditional research paper.",,,,,
SUR.1.3 Software,"A set of instructions, data, or programs used to operate computers and execute specific tasks.",,,,,
SUR.1.4 Updates to datasets and new software versions,"To datasets: the functional process of renewing information already contained in a database or stored elsewhere that results in the creation of a new record and may result in storage of existing data as history. To software: patch, upgrade, or other modification to code that corrects functionality problems in software.",,,,,
SUR.1.5 Data linking,The process of collating and cross-referencing data from different sources in order to create a more valuable and meaningful dataset.,,,,,
SUR.1.6 Persistent identifier,"A long-lasting and unique reference to a digital object of various types (e.g., document, dataset, and webpage). Persistent identifiers (PIDs) are labels that locate, identify, and share information about digital objects. A PID may be connected to a set of metadata describing an object rather than to the object itself.",,,,,
SUR.1.7 Metadata,"Data about data, i.e., data that define and describe the characteristics of other data.",,,,,
SUR.1.8 Integrity of data,The reliability and trustworthiness of data throughout their lifecycle. The assurance that a digital object is uncorrupted and can only be accessed or modified by those authorized to do so.,,,,,
SUR.1.9 Quality measures and assessment vis-à-vis fit for purpose,"The degree to which a dataset meets the requirements for its planned usage as determined by an evaluation of quality metrics (e.g., accuracy, completeness, consistency, and timeliness).",,,,,
SUR.1.10 Peer review of datasets and metadata,An editorial process prior to publication of a dataset whereby people with a similar degree of expertise and experience as the author review and provide input on the integrity and quality of the dataset.,,,,,
SUR.1.11 Reference data/digital objects in journal articles ,"Journals have different guidelines concerning the publication of digital objects, e.g., raw data and software, that accompany a traditional article. Examples of these guidelines are depositing data in a relevant repository, citing a dataset by its PID, and linking the dataset to the article.",,,,,
SUR.1.12 Curation,"The ongoing processing and maintenance of data throughout their lifecycle to ensure long-term accessibility, sharing, and preservation. Data curation is composed of research data management and digital preservation and involves processes such as adding metadata to make data more findable and understandable, ingesting data into a repository, validating file checksums and file fixity checks, and other tasks for organizing, cleaning, describing, enhancing, storing, and preserving data.",,,,,
SUR.1.13 Publisher agreements and policies,"Legal documents that are used to dictate when and how work is published and thereby protect an author's intellectual property from unauthorized use or reproduction. Open access agreements support individual authors to publish open access data at no cost to themselves. Publisher policies are set by the publisher and include, e.g., copyright and licensing, data privacy, and rights and permissions.",,,,,
SUR.1.14 Incentives for data publishing,Staff recognition and rewards for widespread dissemination of research data.,,,,,
SUR.1.15 Mitigation of disincentives for data publishing,"Practices to remove or reduce barriers that limit dissemination of data (e.g., misinterpretation and misuse of data by others, and lack of recognition and effort for sharing).",,,,,
SU2.1.1 Traditional journal article,"A scholarly manuscript submitted to a journal that undergoes a peer review process, an editing and copy-editing process, and finally distribution by publishers able to print and make high-quality scholarly works available to the world. Such manuscripts typically contain analysis and conclusions, but not digital data objects, e.g., raw data and software.",,,,,
SU2.1.2 Supplementary material,"Peer-reviewed material directly relevant to the conclusions of a manuscript that cannot be included in the printed version for reasons of space or medium (e.g., video clips or sound files).",,,,,
SU2.1.3 On request,"Making data available in response to queries typically sent by email. The requester may be required to complete a form, e.g., a data release application agreement.",,,,,
SU2.1.4 Data landing page,"A standalone web page that a person accesses after clicking on a link from an email, ad, or other digital location. For a dataset, such a web page typically includes a narrative description of the dataset and files or links to files pertaining to the dataset, e.g., the dataset itself and the software used to generate the dataset.",,,,,
SU2.1.5 Workflow,"A depiction of a sequence of connected operations or steps that illustrates how data flows through a research data management infrastructure. A workflow includes tasks, people involved, tools (e.g., hardware and software), input, and output for each step.",,,,,
"SU2.1.6 Mainstream media, e.g., newspapers, radio","Traditional means of communication, such as newspapers, television, and radio, that may potentially influence large numbers of people.",,,,,
"SU2.1.7 Social media, e.g., Twitter, Instagram","A catch-all term for a variety of internet applications that allow users to create content and interact with each other, e.g, Twitter, Instagram, Facebook, and Linked-In.",,,,,
SUR.3.1 Citation metrics,"Measures based on the number of times a single entity (e.g., article and dataset) published by a researcher is mentioned in the published work of other authors. Indicator of the quality or importance of a published entity. Citation data are available from citation databases, discipline-specific databases, and through an emerging range of alternative metrics.",,,,,
SUR.3.2 Citation impact,"Quantitative and qualitative tools and methods to measure the impact of an individual's collective work. Quantitative tools include: citation analysis—counting the number of times other authors mention a researcher's published works; the impact factors (IFs) of the journals in which a researcher has published their work (IF is the frequency with which the average article in a journal has been cited in a particular year); and the h-Index for a researcher, which is based on the set of the researcher's most cited papers and the number of citations those papers have received in other authors' publications. Qualitative methods to measure impact include anecdotal evidence.",,,,,
SUR.3.3 Dataset citation,"The practice of referencing data products used in research (e.g., a DOI or key descriptive information about the data, such as the title, source, and responsible parties). Data citation, like the citation of other evidence and sources, is good research practice and is part of the scholarly ecosystem supporting data reuse. (See the Joint Declaration of Data Citation Principles.)",,,,,
SUR.3.4 Provenance,"The historical, attributed, and documented record of a data asset that contains details on its origin—where, when, how, and by whom it was generated/acquired/processed—and on all alterations to the data asset.",,,,,
SUR.3.5 Author identity management,"Use of a persistent, unique, digital researcher identifier such as ORCID to, e.g., track the scholarly outputs of a researcher, assign appropriate author credit, and eliminate author name ambiguity.",,,,,
SUR.3.6 Use of persistent identifiers,The practice of assigning a unique and long-lasting reference that allows for continued access to a data asset.,,,,,
SUR.3.7 Versioning,"The process of numbering different releases of a data asset (e.g., software program and database); the use and management of multiple versions of a document. Version control allows for the ability to revert to a previous revision, which is critical for data traceability, tracking edits, and correcting mistakes.",,,,,
SUR.4.1 Standardized formats,The organization of information according to preset specifications that are agreed upon by formal standards bodies or informal community groups.,,,,,
SUR.4.2 Interoperability tools,"Methods that provide the capability to seamlessly communicate, execute programs, or transfer data among various functional components in a useful and meaningful manner that requires the user to have little or no knowledge of the unique characteristics of those components.",,,,,
SUR.4.3 Discovery platform,Software systems that use metadata to identify and recommend sources of data or other digital objects.,,,,,
SUR.4.4 Catalog,"Completely organized services that enable any user, e.g., analysts, data scientists and developers, to discover, explore, and use data assets.",,,,,
SUR.4.5 Registries of repositories,"Databases containing information about trusted repositories that are provided by the repository managers and are useful for human and machine users, e.g., the Re3data Repository Registry and the NIST Materials Resource Registry.",,,,,
SUR.5.1 Internal access,"The ability of individuals in an organization to view and retrieve data and other digital objects that were generated, collected, or processed by an individual or group in the same organization.",,,,,
SUR.5.2 External access,"The ability of individuals in organizations other than the organization that generated, collected, or processed the data and other digital objects to view and retrieve such digital resources.",,,,,
"SUR.5.3 Programmatic access, i.e., API","The ability of a user to view and retrieve data made possible by an Application Programming Interface (API), which is a set of protocols, routines, functions and/or commands that programmers use to facilitate interaction between distinct software services.",,,,,
SUR.5.4 Virtual and physical enclaves,"Secure networks through which confidential data, such as personally identifiable information from census data, can be stored and disseminated. In a virtual data enclave a researcher can access data from their own computer but cannot download or remove the data from the remote server. Higher security data can be accessed through a physical data enclave wherein a researcher is required to access data from a monitored room where the data are stored on non-networked computers.",,,,,
SUR.5.5 Access vs visiting,"Data visiting is an approach whereby sensitive data stays under the control of the owner and consumers (e.g., analysts or machine learning algorithms) are permitted to work with the data on location. With data access, users can store, retrieve, move, or manipulate stored data.",,,,,
SUR.5.6 Availability statement,"A declaration letting a user know where and how to access data that support the results and analyses of a published study. A declaration may include links to publicly accessible datasets that were analyzed or generated during the study, descriptions of what data are available and/or information on how to access data that are not publicly available.",,,,,
SUR.5.7 Mitigation of barriers and economic constraints,Practices that reduce or eliminate programmatic and administrative constraints and transactional costs of accessing data.,,,,,
SUR.6.1 Ownership,"The act of having legal rights and complete control over data assets. Ownership defines and provides information about the rightful owner of data assets and the acquisition, use, and distribution policy implemented by the data owner.",,,,,
"SUR.6.2 Encouragement and support for sharing, use, and reuse",Incentives and human and infrastructural resources that increase the quantity and quality of data assets for access and dissemination.,,,,,
SUR.6.3 Indigeneous data rights,"Indigenous data sovereignty (IDS) refers to the right of Indigenous peoples to govern the collection, ownership, and application of data about Indigenous communities, peoples, lands, and resources. IDS encompasses data, information, and knowledge about Indigenous individuals, collectives, entities, lifeways, cultures, lands, and resources.",,,,,
SUR.6.4 Intellectual property rights/restrictions,"Intellectual property (IP) is something of value (an asset) that is created from an original idea, invention, or creative expression. IP rights are legally enforceable claims for owners of such items, including data products (e.g., software). An IP agreement must include an assessment of what IP rights subsist in the data, who owns them, what exceptions or limitations apply, and any contractual rights or policies related to IP that should be considered within the data governance framework, including acquired and generated data as well as “background” (i.e., pre-existing) and “foreground” (i.e., from original research) IP.",,,,,
SUR.6.5 Usage agreements/terms/licenses and required permissions,Usage agreements: legally binding contracts between an originator of a digital object and a user of the object that spell out the rights and responsibilities of all involved parties. User licenses: written contracts that give a user permission to work on another party's digital object under a certain set of conditions and typically requires that the user pay a royalty fee.,,,,,
SUR.6.6 Data sharing agreements and licensing,Sharing agreements: formal contracts that detail what data are being shared and the appropriate use of the data and include provisions concerning access and dissemination. Licensing agreements: documents that describe what kind of data are being shared with a user and clearly state the purpose and duration of access being provided to the user along with restrictions and security protocols that the user of the data must follow.,,,,,
SUR.6.7 Service-level agreements,Contracts between two parties that define and measure the level of service a data provider will deliver to a user. The agreements aim to define expectations of the level of service and quality between data providers and users.,,,,,
SUR.6.8 Terms of service ,Legal agreements between a data service provider and a user that detail the set of rules and regulations a provider attaches to a software service or web-delivered product.,,,,,
"SUR.6.9 Standardized, machine-actionable license documents","Written contracts in a common, agreed-upon form that can be read, understood, and implemented by a computer. Such contracts give a user permission to use a creator's digital object under a certain set of conditions.",,,,,
SUR.6.10 Citation requirements,"References to data and other digital objects that are mandated by a data provider, formal agreement, or publishing entity.",,,,,
SUR.7.1 Unclassified but sensitive information,"A designation of information (data) in the US federal government that is not classified for national security reasons, but that warrants or requires administrative control and protection from public or other unauthorized disclosure for other reasons. Personally identifiable information (PII), e.g., an individual's birthdate, address, and phone number, and Business Identifiable Information (BII), e.g., trade secrets and financial information, fit this designation. The US government uses the term “controlled unclassified information (CUI).""",,,,,
SUR.7.2 Security classification,"A term typically associated with U.S. federal government national security information. NIST has developed a broader document that addresses security controls, defined as the safeguards or countermeasures employed within a system or an organization to protect the confidentiality, integrity, and availability of the system and its information and to manage information security risk.",,,,,
SUR.7.3 Protection of limited data/secure platforms/enclaves,"Limited data: in healthcare, a set of identifiable healthcare information that the HIPAA Privacy Rule permits covered entities to share with certain entities for research purposes if certain conditions are met. Data security platform: aggregates data protection requirements across data types, storage silos, and ecosystems to create an organization-wide data security solution. Secure data enclave: a system that allows data owners to control data access and ensure data security while facilitating approved uses of data by other parties.",,,,,
SUR.7.4 Constraints and restrictions on data use and sharing,"Technical, administrative, or legal limitations on the use and sharing of data.",,,,,
SUR.7.5 Anonymization,A process of preserving private or confidential information by deleting or encoding identifiers that link individuals and stored data.,,,,,
"SUR.8.1 Extensibility across communities, including machine-based interactions","A measure of the ability to expand a research data management architecture to enable interactions with a broad group of stakeholders and types of equipment, achieved by adding new functionality or modifying existing functionality.",,,,,
SUR.8.2 Capture of insights from ML and use of these to improve datasets for future AI applications,"Recording and retaining information obtained via computer systems that use algorithms and statistical models to enable understanding of complex problems, and employing such understanding to develop enhanced datasets for new AI solutions.",,,,,
SUR.8.3 Capture of data performance characteristics,"Recording and retaining information concerning the quality attributes of a dataset, e.g., validity, accuracy, completeness, relevance, uniformity and consistency.",,,,,
SUR.8.4 Location of data,"Methods whereby, and systems and devices wherein, data are saved and from which data can be retrieved, e.g., on premises, cloud, temporary cache, and removable media.",,,,,
SUR.8.5 Migration strategies and mitigation against data loss,"Approaches and practices to eliminate, prevent, or reduce the intentional or unintentional destruction or disappearance of information caused by people, processes, or other means.",,,,,
SUR.8.6 Economic impact of reuse,Monetary benefits of using existing data compared to re-generating identical data.,,,,,
PD.1.1 Use,"Instances wherein datasets are utilized for meaningful purposes, e.g., problem-solving and decision-making.",,,,,
PD.1.2 Impact,"Demonstrated, positive outcomes attributed to use of a dataset, e.g., a scientific discovery and a new measurement instrument or product.",,,,,
PD.1.3 Value,"Merit or worth of data in terms of their usefulness and fitness for purpose, e.g., to make sound, fact-based conclusions and decisions.",,,,,
PD.1.4 Uniqueness,"The quality of being unlike any other data in terms of, e.g., type and characteristics.",,,,,
PD.1.5 Cost,Financial resources required to store and preserve data.,,,,,
PD.1.6 Provenance,"The historical, attributed, and documented record of a data asset that contains details on its origin—where, when, how, and by whom it was generated/acquired/processed—and on all alterations to the data asset.",,,,,
PD.1.7 Legal and regulatory,"Requirements via contract, law, regulation, or other agreement to preserve data.",,,,,
PD.2.1 Longevity and support ,The amount of time a dataset is retained in an organization and the resources to maintain this retention.,,,,,
PD.2.2 Funding models,Approaches to build a reliable funding base that will support an organization's core research data projects and services.,,,,,
PD.2.3 Business models,Approaches to describe how an organization ensures that its research data projects and services provide value.,,,,,
PD.3.1 Media to store and preserve data,Devices and cloud services used to retain data in the short-term and long-term.,,,,,
PD.3.2 File integrity,"The process of protecting a file from unauthorized changes or environmental hazards, i.e., validation to determine if a file has been altered after its creation, curation, archiving, or other qualifying event.",,,,,
PD.3.3 Ability to do advanced searches,"Capability to narrow a query through, e.g., the use of filters that eliminate irrelevant information and enable the identification of desired content.",,,,,
PD.3.4 Backup and recovery,"Backup: the process of making copies of data or data files to use in the event the original data or data files are lost or destroyed. Recovery: the process of restoring data that have been lost, accidentally deleted, corrupted, or made inaccessible for any reason.",,,,,
PD.4.1 Roles and responsibilities ,The job functions and obligations that enable the movement of data among organizations.,,,,,
PD.4.2 Registry maintenance and curation,"The processes of harvesting, organizing, and handling a collection of data-related resources such as repositories, services, and software, to facilitate ease of user searches and retrieval of information. Examples of registries are re3data and the NIST Materials Resource Registry.",,,,,
PD.4.3 Disciplinary archives,A place to store data from a specific field of study or branch of knowledge that is important but that doesn't need to be accessed or modified frequently (if at all).,,,,,
"PD.5.1 Technical decisions, e.g., data archiving","Conclusions regarding retention and disposition of research data that are based on scientific considerations such as merit and future potential usefulness of the data, e.g., data archiving.",,,,,
PD.5.2 Administrative/policy decisions,"Conclusions regarding retention and disposition of research data that are based on logistical or operational considerations, e.g., cost of data archiving.",,,,,
PD.5.3 Deaccessioning/end-of-life,"The formal, documented removal of a data collection or dataset from its location or custody of an archive service.",,,,,
PD.5.4 Legal documents,Schedules for retention and disposition of data set by formal contracts or other agreements.,,,,,
PD.5.5 End-of-life special considerations,Any actions taken before disposition of data that has reached the end of its useful life or will no longer receive support for archiving. An example consideration is adhering to security protocols for sensitive data.,,,,,
"PD.5.6 Recognition of removed data, i.e., tombstone page","Creation of a special type of landing page (i.e., tombstone page) describing the data that have been removed and providing a full bibliographic citation, a DOI (if one has been assigned), and a statement on unavailability detailing the circumstances that led to removal of the data.",,,,,
